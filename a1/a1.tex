\documentclass[10pt]{article}
\usepackage[left=1.5cm, right=1.5cm, top=0.8in, bottom=0.7in]{geometry}
\usepackage{fancyhdr}
\pagestyle{fancy}
\usepackage[most,breakable]{tcolorbox}
\usepackage{pdfcol,xcolor}
\usepackage{tikz}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
%\usepackage{url}
\usepackage{dsfont}
\usepackage{amssymb,amsmath}
\usepackage{xspace}
\usepackage[normalem]{ulem}
\usepackage{bm}
\usepackage[breaklinks=true,colorlinks,linkcolor=magenta,urlcolor=magenta,citecolor=black]{hyperref}
\usepackage{cleveref}
\usepackage{xpatch}
\xpretocmd{\algorithm}{\hsize=\linewidth}{}{}

\newtcolorbox[auto counter]{exercise}[1][]{%
colback=yellow!10,colframe=red!75!black,coltitle=white,use color stack,enforce breakable,enhanced,fonttitle=\bfseries,before upper={\parindent15pt\noindent}, title={\color{white}Exercise~\thetcbcounter: #1}}
\pagecolor{yellow!10}

\lhead{
\textbf{University of Waterloo}
}
\rhead{\textbf{2024 Spring}
}
\chead{\textbf{
CS480/680
}}
\lfoot{}
\cfoot{\textbf{Yao-Liang Yu (yaoliang.yu@uwaterloo.ca) \textcopyright 2024}}

\newcommand{\RR}{\mathds{R}}
\newcommand{\sign}{\mathop{\mathrm{sign}}}
\newcommand{\argmin}{\mathop{\mathrm{argmin}}}
\newcommand{\zero}{\mathbf{0}}
\newcommand{\one}{\mathbf{1}}
\newcommand{\bv}{\mathbf{b}}
\newcommand{\wv}{\mathbf{w}}
\newcommand{\xv}{\mathbf{x}}
\newcommand{\Xv}{\mathbf{X}}
\newcommand{\Yv}{\mathbf{Y}}
\newcommand{\zv}{\mathbf{z}}
\newcommand{\yv}{\mathbf{y}}
\newcommand{\rv}{\mathbf{r}}
\newcommand{\inner}[2]{\langle #1, #2 \rangle}
\newcommand{\red}[1]{{\color{red}#1}}
\newcommand{\blue}[1]{{\color{blue}#1}}
\newcommand{\magenta}[1]{{\color{magenta}#1}}
\newcommand{\green}[1]{{\color{green}#1}}
\newcommand{\xbs}{\bm{\mathsf{x}}}
\newcommand{\wbs}{\bm{\mathsf{w}}}
\newcommand{\zbs}{\bm{\mathsf{z}}}
\newcommand{\gbs}{\bm{\mathsf{g}}}
\newcommand{\EE}{\mathds{E}}
\newcommand{\argmax}{\mathop{\mathrm{argmax}}}
\newcommand{\Ysf}{\mathsf{Y}}
\newcommand{\Xsf}{\mathsf{X}}
\newcommand{\ea}{{et al.}\xspace}
\newcommand{\eg}{{e.g.}\xspace}
\newcommand{\ie}{{i.e.}\xspace}
\newcommand{\iid}{{i.i.d.}\xspace}
\newcommand{\cf}{{cf.}\xspace}
\newcommand{\wrt}{{w.r.t.}\xspace}
\newcommand{\aka}{{a.k.a.}\xspace}
\newcommand{\etc}{{etc.}\xspace}
\newcommand{\sgm}{\mathsf{sgm}}
\newcommand{\Dc}{\mathcal{D}}
\newcommand{\pred}[1]{[\![#1]\!]}
\newcommand{\ans}[1]{{\color{orange}\textsf{Ans}: #1}}


%===========================================================
\begin{document}

\begin{center}
  \large{\textbf{CS480/680: Introduction to Machine Learning} \\ Homework 1\\ \red{Due: 11:59 pm, May 29, 2024}, \red{submit on LEARN}.} \\

  {\bf \green{Jiaze Xiao}} \\
  {\bf \green{20933691}}

\end{center}

\begin{center}
  Submit your writeup in pdf and all source code in a zip file (with proper documentation). Write a script for each programming exercise so that the TA can easily run and verify your results. Make sure your code runs!

  [Text in square brackets are hints that can be ignored.]
\end{center}



\begin{exercise}[Perceptron (8 pts)]
  \blue{\textbf{Convention:} All algebraic operations, when applied to a vector or matrix, are understood to be element-wise (unless otherwise stated).}

  \begin{algorithm}[H]
    \DontPrintSemicolon
    \KwIn{$X\in\RR^{d\times n}$, $\yv\in \{-1,1\}^n$, $\wv=\zero_d$, $b=0$, $\mathsf{max\_pass} \in \mathds{N}$}

    \KwOut{$\wv, b, mistake$}

    \For{$t=1, 2, \ldots, \mathsf{max\_pass}$ }{
      $mistake(t) \gets 0$

      \For{$i=1, 2, \ldots, n$}{
        \If{$y_i (\inner{\xv_i}{\wv}+b) \leq 0$}{
          $\wv \gets \wv + y_i\xv_i$ \tcp*{$\xv_i$ is the $i$-th column of $X$}

          $b \gets b + y_i$

          $mistake(t) \gets mistake(t) + 1$
        }
      }
    }
    \caption{The perceptron.}
    \label{alg:perceptron}
  \end{algorithm}

  \begin{enumerate}
    \item (1 pt)  \uline{Implement} the perceptron in \Cref{alg:perceptron}. Your implementation should take input as $X = [\xv_1, \ldots, \xv_n] \in \RR^{d \times n}$, $\yv \in \{-1,1\}^{n}$, an initialization of the hyperplane parameters $\wv\in\RR^{d}$ and $b\in \RR$, and the maximum number of passes of the training set [suggested $\mathsf{max\_pass} = 500$]. \uline{Run} your perceptron algorithm on the \href{https://archive.ics.uci.edu/ml/datasets/spambase}{\textsf{spambase}} dataset (available on \href{https://cs.uwaterloo.ca/~y328yu/mycourses/480/assignment.html}{course website}), and \uline{plot the number of mistakes ($y$-axis) \wrt the number of passes ($x$-axis)}.

          \begin{center}
            \includegraphics[scale=0.7]{ex1-perceptron/perceptron_mistakes_vs_passes.png}
          \end{center}
          (run \texttt{python3 q1.py} under a1/ex1-perceptron)

    \item (1 pt) Using the one-vs-all reduction to \uline{implement} a multiclass perceptron. You may call your binary implementation. \uline{Test} your algorithm on the \href{https://archive-beta.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones}{activity} dataset (available on \href{https://cs.uwaterloo.ca/~y328yu/mycourses/480/assignment.html}{course website}), and \uline{report your final errors on the training and test sets}.

          \ans\\
          \leavevmode\\
          Training Error: 1.89\%\\
          Test Error: 4.92\%\\
          \leavevmode\\
          (run \texttt{python3 q2.py} under a1/ex1-perceptron)
          {\vskip0.5cm}

    \item (1 pt) Using the one-vs-one reduction to \uline{implement} a multiclass perceptron. You may call your binary implementation. \uline{Test} your algorithm on the \href{https://archive-beta.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphoness}{activity} dataset (available on \href{https://cs.uwaterloo.ca/~y328yu/mycourses/480/assignment.html}{course website}), and \uline{report your final errors on the training and test sets}.

          \ans\\
          \leavevmode\\
          Training Error: 1.51\%\\
          Test Error: 5.02\%\\
          \leavevmode\\
          (run \texttt{python3 q3.py} under a1/ex1-perceptron)
          {\vskip0.5cm}

    \item (2 pts) Consider the (continuous) piece-wise function
          \begin{align}
            f(\wv) := \max_k f_k(\wv),
          \end{align}
          where each $f_k$ is \href{https://en.wikipedia.org/wiki/Differentiable_function}{continuously differentiable}.
          We define the \href{https://en.wikipedia.org/wiki/Subderivative}{derivative} of $f$ at any $\wv$ as follows: first find (any) $k$ such that $f(\wv) = f_k(\wv)$, \ie, $f_k(\wv)$ achieves the maximum among all pieces; then we let $f'(\wv) = f_k'(\wv)$. [Clearly, the index $k$ that achieves maximum may depend on $\wv$, the point we evaluate the derivative at.] Now consider the following problem [padding applied, $y_i \in \{\pm1\}$]:
          \begin{align}
            \label{eq:bp}
            \min_{\wbs} ~ \sum_{i=1}^n \max\{ 0, -y_i (\inner{\xbs_i}{\wbs}) \}.
          \end{align}
          \uline{Prove} that in each iteration, the (binary) perceptron algorithm essentially picks a term from the above summation, computes the corresponding derivative (say $\gbs$), and performs a gradient update:
          \begin{align}
            \wbs \gets \wbs - \gbs.
          \end{align}
          [You may ignore the degenerate case when $\inner{\xv_i}{\wbs} = 0$, and you can use  the usual \href{https://en.wikipedia.org/wiki/Chain_rule}{chain rule} for our derivative.]

          \ans\\
          \leavevmode\\
          The objective function to minimize is:
          $$
            \sum_{i=1}^n \max\{ 0, -y_i (\inner{\xbs_i}{\wbs}) \}.
          $$

          This is a piece-wise function, where each term $\max\{ 0, -y_i (\inner{\xbs_i}{\wbs}) \}$ is differentiable everywhere except at the point where $-y_i (\inner{\xbs_i}{\wbs}) = 0$ which is ignored. Let's define each piece $f_i(\wbs)$ as follows:
          $$
            f_i(\wbs) = \max\{ 0, -y_i (\inner{\xbs_i}{\wbs}) \}.
          $$

          which can be written as:
          $$
            f_i(\wbs) =
            \begin{cases}
              0                           & \text{if } -y_i (\inner{\xbs_i}{\wbs}) \le 0, \\
              -y_i (\inner{\xbs_i}{\wbs}) & \text{if } -y_i (\inner{\xbs_i}{\wbs}) > 0.
            \end{cases}
          $$

          Derivative of Each Piece
          \begin{itemize}
            \item Case 1: If $-y_i (\inner{\xbs_i}{\wbs}) \le 0$, then $f_i(\wbs) = 0$, and the derivative is $\nabla f_i(\wbs) = 0$.
            \item Case 2: If $-y_i (\inner{\xbs_i}{\wbs}) > 0$, then $f_i(\wbs) = -y_i (\inner{\xbs_i}{\wbs})$, and the derivative is:
                  \begin{equation*}
                    \begin{aligned}
                      f_i(\wbs+\zbs)-f_i(\wbs) & =-y_i (\inner{\xbs_i}{\zbs}) \\
                      \nabla f_i(\wbs)(\zbs)   & =-y_i (\inner{\xbs_i}{\zbs}) \\
                      \nabla f_i(\wbs)         & = -y_i \xbs_i.
                    \end{aligned}
                  \end{equation*}
          \end{itemize}
          In perceptron algorithm, at each iteration, if $\xbs_i$ is misclassified by $\wbs$, the perceptron updates $\wbs$ as follows:
          $$
            \wbs \gets \wbs + y_i \xbs_i.
          $$

          This can be interpreted as follows:
          1. Identify a misclassified point $\xbs_i$.
          2. Compute the gradient of the corresponding term $\max\{ 0, -y_i (\inner{\xbs_i}{\wbs}) \}$, which is $\nabla f_i(\wbs) = -y_i \xbs_i$ if $y_i (\inner{\xbs_i}{\wbs}) \le 0$.
          3. Perform the gradient update:
          $$
            \wbs \gets \wbs - \nabla f_i(\wbs) = \wbs - (-y_i \xbs_i) = \wbs + y_i \xbs_i.
          $$
          Therefore, this shows that the perceptron algorithm is performing a gradient update on the objective function given in Equation $\eqref{eq:bp}$.
            {\vskip0.5cm}

    \item (1 pt) Consider the following problem, where \red{$y_i \in \{1, 2, \ldots, c\}$}:
          \begin{align}
            \label{eq:mp}
            \min_{\wbs_1, \ldots, \wbs_c} ~ \sum_{i=1}^n \max_{k=1,\ldots, c} \Big [\inner{\xbs_i}{\wbs_k} - \inner{\xbs_i}{\wbs_{y_i}} \Big].
          \end{align}
          \uline{Show} that when $c=2$, we reduce to the binary perceptron problem in \eqref{eq:bp}. [Try to identify the weights $\wbs$, using some transformation.]

          \ans\\
          \leavevmode\\
          When $c=2$, the expression becomes:
          $$
            \min_{\wbs_1, \wbs_2} ~ \sum_{i=1}^n \max \left\{ \inner{\xbs_i}{\wbs_1} - \inner{\xbs_i}{\wbs_{y_i}}, \inner{\xbs_i}{\wbs_2} - \inner{\xbs_i}{\wbs_{y_i}} \right\}.
          $$

          Let $f_i(\wbs)=\max \left\{ \inner{\xbs_i}{\wbs_1} - \inner{\xbs_i}{\wbs_{y_i}}, \inner{\xbs_i}{\wbs_2} - \inner{\xbs_i}{\wbs_{y_i}} \right\}$ which can be simplified as:
          $$
            f_i(\wbs) =
            \begin{cases}
              \max \left\{ 0, \inner{\xbs_i}{\wbs_2} - \inner{\xbs_i}{\wbs_1} \right\} & \text{ if } y_i = 1, \\
              \max \left\{0, \inner{\xbs_i}{\wbs_1} - \inner{\xbs_i}{\wbs_2}\right\}   & \text{ if } y_i = 2.
            \end{cases}
          $$

          To transform this into the binary perceptron problem, define:
          $$
            \wbs = \wbs_1 - \wbs_2.
          $$

          If $y_i = 1$, we have:
          $$
            \max \left\{ 0, \inner{\xbs_i}{\wbs_2} - \inner{\xbs_i}{\wbs_1} \right\} = \max \left\{ 0, -\inner{\xbs_i}{\wbs} \right\}.
          $$
          If $y_i = 2$, we have:
          $$
            \max \left\{ 0,\inner{\xbs_i}{\wbs_1} - \inner{\xbs_i}{\wbs_2}\right\} = \max \left\{ 0,\inner{\xbs_i}{\wbs}\right\}.
          $$

          Combining the above:
          $$
            f_i(\wbs)=\max \left\{ 0, -\overline{y_i} \inner{\xbs_i}{\wbs} \right\},
          $$
          where $\overline{y_i} \in \{\pm1\}$ with the following encoding:
          \begin{itemize}
            \item $\overline{y_i} = 1$ if the original $y_i = 1$.
            \item $\overline{y_i} = -1$ if the original $y_i = 2$.
          \end{itemize}

          Thus, we reduce the multiclass problem for $c=2$ to the binary perceptron problem:
          $$
            \min_{\wbs} \sum_{i=1}^n \max \left\{ 0, -\overline{y_i} \inner{\xbs_i}{\wbs} \right\}.
          $$
          {\vskip0.5cm}

    \item (2 pts) Based on the analogy to the binary case, \uline{develop and implement} a multiclass perceptron algorithm to solve \eqref{eq:mp} directly. \uline{Run} your implementation on the \href{https://archive-beta.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones}{activity} dataset (available on \href{https://cs.uwaterloo.ca/~y328yu/mycourses/480/assignment.html}{course website}) and \uline{report the final errors on the training and test sets}. [Hint: obviously, we want to predict as follows: $\hat y = \argmax\limits_{k=1, \ldots, c}~ \inner{\xbs}{\wbs_k}$, \ie, the class $k$ whose corresponding $\wbs_k$ maximizes the inner product. Explain your algorithm (\eg, through pseudo-code).]

          \ans

          \begin{algorithm}[H]
            \DontPrintSemicolon
            \KwIn{$X\in\RR^{n \times d}$, $\yv\in \{1, 2, \ldots, c\}^n$, $\mathsf{max\_pass} \in \mathds{N}$}

            \KwOut{$\{\wbs_k\}_{k=1}^c$}
            \For{$k=1, 2, \ldots, c$ }{
              $\wbs_k \gets \zero_{d+1}$
            }
            \For{$i=1, 2, \ldots, n$ }{
              $X_i.append(1)$
            }
            \For{$t=1, 2, \ldots, \mathsf{max\_pass}$ }{
              \For{$i=1, 2, \ldots, n$}{
                $\hat y = \argmax\limits_{k=1, \ldots, c}~ \inner{\xbs}{\wbs_k}$

                \If{$\hat{y} \neq y_i$}{
                  $\wbs_{y_i} \gets \wbs_{y_i} + \xbs_i$

                  $\wbs_{\hat{y}} \gets \wbs_{\hat{y}} - \xbs_i$
                }
              }
            }
            \caption{Multiclass Perceptron}
            \label{alg:multiclass_perceptron}
          \end{algorithm}

          Training Error: 1.71\%\\
          Test Error: 4.48\%\\
          \leavevmode\\
          (run \texttt{python3 q6.py} under a1/ex1-perceptron)
          {\vskip0.5cm}
  \end{enumerate}
\end{exercise}

\newpage
\begin{exercise}[Generalized linear models (6 pts)]
  Recall that in logistic regression we assumed the \emph{binary} label $\Ysf_i \in \{0,1\}$ follows the Bernoulli distribution: $\Pr(\Ysf_i = 1 | \Xsf_i) = p_i$, where $p_i$ also happens to be the mean. Under the independence assumption we derived the (conditional) negative log-likelihood function:
  \begin{align}
    -\sum_{i=1}^n (1-y_i) \log(1-p_i) + y_i \log(p_i).
  \end{align}
  Then, we parameterized the mean parameter $p_i$ through the logit transform:
  \begin{align}
    \log\frac{p_i}{1-p_i} = \inner{\xv_i}{\wv} + b, \quad \mbox{ or equivalently } \quad p_i = \frac{1}{1+\exp(-\inner{\xv_i}{\wv} - b)}.
  \end{align}
  Lastly, we found the weight vector $\wv$ and $b$ by minimizing the negative log-likelihood function.

  In the following we generalize the above idea significantly. Let the (conditional) density of $\Ysf$ (given $\Xsf= \xv$) be
  \begin{align}
    \label{eq:GLM}
    p(y| \xv) = \exp\Big[ \mu(\xv)\cdot y - \lambda(\xv) \Big] \cdot q(y),
  \end{align}
  where $\mu:\RR^d \to \RR$ is a function of $\xv$ and $\lambda(\xv) = \log \int_y \exp\big( \mu(\xv) \cdot y \big)  q(y) \mathrm{d} y$ so that $p(y|\xv)$ is properly normalized wrt $y$ (i.e., integrate to 1). For discrete $y$ (such as in logistic regression), replace the density with the \href{https://en.wikipedia.org/wiki/Probability_mass_function}{probability mass function} and the integral with sum.

  \red{As always, you need to supply sufficient derivation details to justify your final answer.}

  \begin{enumerate}
    \item (1 pt) Given a dataset $\{(\xv_i, y_i)\}_{i=1}^n$, \uline{derive the (conditional) negative log-likelihood function} of $y_1, \ldots, y_n$, assuming independence and the density form in \eqref{eq:GLM}.

          \ans\\
          \begin{equation*}
            \begin{aligned}
              \Pr(\Ysf_1 = y_1, \ldots, \Ysf_n = y_n | \Xsf_1 = \xv_1, \ldots, \Xsf_n = \xv_n) = & \prod_{i=1}^n p(y_i | \xv_i)                                                     \\
              =                                                                                  & \prod_{i=1}^n \exp\Big[ \mu(\xv_i) \cdot y_i - \lambda(\xv_i) \Big] \cdot q(y_i)
            \end{aligned}
          \end{equation*}

          Taking the negative log-likelihood, we obtain:
          \begin{equation*}
            \begin{aligned}
                & -\log \left( \prod_{i=1}^n \exp\Big[ \mu(\xv_i) \cdot y_i - \lambda(\xv_i) \Big] \cdot q(y_i) \right)                 \\
              = & -\sum_{i=1}^n \log \left( \exp\Big[ \mu(\xv_i) \cdot y_i - \lambda(\xv_i) \Big] \cdot q(y_i) \right)                  \\
              = & -\sum_{i=1}^n \left( \log \left( \exp\Big[ \mu(\xv_i) \cdot y_i - \lambda(\xv_i) \Big] \right) + \log(q(y_i)) \right) \\
              = & -\sum_{i=1}^n \left( \mu(\xv_i) \cdot y_i - \lambda(\xv_i) + \log(q(y_i)) \right)
            \end{aligned}
          \end{equation*}

          Therefore, the (conditional) negative log-likelihood function of $y_1, \ldots, y_n$ given $\xv_1, \ldots, \xv_n$ is:

          \begin{align}
            \boxed{ \sum_{i=1}^n (-\mu(\xv_i) \cdot y_i + \lambda(\xv_i)  - \log(q(y_i))) }
          \end{align}
          \newpage
    \item (1 pt) Plug the usual linear parameterization
          \begin{align}
            \mu(\xv) = \inner{\xv}{\wv} + b = \inner{\xbs}{\wbs}
          \end{align}
          into your (conditional) \uline{negative log-likelihood} and \uline{compute the gradient of the resulting function}. [Hint: you may \href{https://en.wikipedia.org/wiki/Leibniz_integral_rule}{swap differentiation with integral} and your gradient may involve implicitly defined terms.]

          \ans\\
          \leavevmode\\
          Plug the linear parameterization $\mu(\xv) = \inner{\xv}{\wv} + b = \inner{\xbs}{\wbs}$ into the negative log-likelihood function:
          \begin{align}
            \boxed{\ell_n(\wbs) = \sum_{i=1}^n -\inner{\xbs_i}{\wbs} \cdot y_i + \lambda(\xv_i)  - \log(q(y_i))}
          \end{align}
          First, consider the term $-\inner{\xbs_i}{\wbs} \cdot y_i$:
          $$
            \nabla_{\wbs} \left( -\inner{\xbs_i}{\wbs} \cdot y_i \right) = -y_i \xbs_i.
          $$

          Next, we need to differentiate $\lambda(\xv_i)$.
          $$
            \lambda(\xv_i) = \log \int_y \exp\big( \inner{\xbs_i}{\wbs} \cdot y \big) q(y) \mathrm{d} y.
          $$

          By the Leibniz integral rule, the gradient of $\lambda(\xv_i)$ with respect to $\wbs$ is:
          \begin{equation*}
            \begin{aligned}
              \nabla_{\wbs} \lambda(\xv_i) = & \frac{\int_y \frac{\partial}{\partial\wbs}\exp\big( \inner{\xbs_i}{\wbs} \cdot y \big)  q(y) \mathrm{d} y}{\int_y \exp\big( \inner{\xbs_i}{\wbs} \cdot y \big) q(y) \mathrm{d} y} \\
              =                              & \frac{\int_y \exp\big( \inner{\xbs_i}{\wbs} \cdot y \big) y \xbs_i q(y) \mathrm{d} y}{\int_y \exp\big( \inner{\xbs_i}{\wbs} \cdot y \big) q(y) \mathrm{d} y}                      \\
              =                              & \frac{\int_y \exp\big( \inner{\xbs_i}{\wbs} \cdot y \big) y \xbs_i q(y) \mathrm{d} y}{\exp(\log\int_y \exp\big( \inner{\xbs_i}{\wbs} \cdot y \big) q(y) \mathrm{d} y)}            \\
              =                              & \int_y \frac{\exp\big( \inner{\xbs_i}{\wbs} \cdot y \big) y \xbs_i q(y)}{\exp(\lambda(\xbs_i))} \mathrm{d} y                                                                      \\
              =                              & \xbs_i\int_y yp(y| \xbs_i)\mathrm{d} y                                                                                                                                            \\
              =                              & \xbs_i\mathbb{E}[y | \xbs_i]
            \end{aligned}
          \end{equation*}

          Putting it all together, the gradient of $\ell_n(\wbs)$ is:
          \begin{align}
            \boxed{\nabla \ell_n(\wbs) = \sum_{i=1}^n  ( \mathbb{E}[y  | \xbs_i]-y_i)\xbs_i}
          \end{align}

          \newpage
    \item (1 pt) Let us revisit linear regression, where
          \begin{align}
            p(y | \xv) =\tfrac{1}{\sqrt{2\pi}}\exp\big(-\tfrac{(y - \nu(\xv))^2}{2}\big)
          \end{align}
          \uline{Identify the functions $\mu(\xv)$, $\lambda(\xv)$ and $q(y)$} for the above specialization. Based on the linear parameterization in Ex 2.2, derive the \uline{negative log-likelihood} and \uline{gradient}. [Hint: you may simply plug into the more general result in Ex 2.2. Compare with what you already learned about linear regression to make sure both Ex 2.2 and Ex 2.3 are correct.]

          \ans\\
          \leavevmode\\
          The given density can be rewritten in exponential form:
          \begin{equation*}
            \begin{aligned}
              p(y | \xv) = & \exp\left(-\frac{(y - \nu(\xv))^2}{2} - \log \sqrt{2\pi}\right)                       \\
              =            & \exp\left(y \nu(\xv) - \frac{\nu(\xv)^2}{2} - \frac{y^2}{2} - \log \sqrt{2\pi}\right)
            \end{aligned}
          \end{equation*}

          According to \eqref{eq:GLM}, we identify:
          \begin{align}
            \boxed{\mu(\xv) = \nu(\xv)}
          \end{align}

          The term $\lambda(\xv)$ depends on $\xv$:
          \begin{align}
            \boxed{\lambda(\xv) = \frac{\nu(\xv)^2}{2} }
          \end{align}
          The remaining terms that are independent of $\xv$ fall into $q(y)$:
          \begin{align}
            \boxed{q(y) = \exp\left(-\frac{y^2}{2}-\log \sqrt{2\pi}\right)=\frac{1}{\sqrt{2\pi}}\exp\left(-\frac{y^2}{2}\right)}
          \end{align}

          Negative Log-likelihood:
          \begin{equation*}
            \begin{aligned}
              \ell_n(\wbs) = & -\sum_{i=1}^n \log p(y_i | \xv_i)                                                                      \\
              =              & -\sum_{i=1}^n \log \left(\frac{1}{\sqrt{2\pi}} \exp\left(-\frac{(y_i - \nu(\xv_i))^2}{2}\right)\right) \\
              =              & -\sum_{i=1}^n \left( -\frac{(y_i - \nu(\xv_i))^2}{2} - \log \sqrt{2\pi} \right)
            \end{aligned}
          \end{equation*}
          For the linear parameterization $\nu(\xv_i) = \inner{\xbs_i}{\wbs}$, the negative log-likelihood becomes:

          \begin{align}
            \boxed{\ell_n(\wbs) = \sum_{i=1}^n \left( \frac{(y_i - \inner{\xbs_i}{\wbs})^2}{2} + \log \sqrt{2\pi} \right)}
          \end{align}

          Thus,
          \begin{equation*}
            \begin{aligned}
              \nabla \ell_n(\wbs) = & \frac{\partial}{\partial \wbs}\sum_{i=1}^n \left( \frac{(y_i - \inner{\xbs_i}{\wbs})^2}{2} + \log \sqrt{2\pi} \right) \\
              =                     & \sum_{i=1}^n (y_i - \inner{\xbs_i}{\wbs}) \cdot (-\xbs_i)
            \end{aligned}
          \end{equation*}

          Therefore:

          \begin{align}
            \boxed{\nabla \ell_n(\wbs) = \sum_{i=1}^n (\inner{\xbs_i}{\wbs}-y_i) \xbs_i}
          \end{align}
          {\vskip0.5cm}
    \item (1 pt) Let us revisit logistic regression, where
          \begin{align}
            \Pr( \Ysf = y | \xv) =[\nu(\xv)]^{y} [1-\nu(\xv)]^{1-y}, ~~\mbox{ where } ~~ y \in \{0, 1\}.
          \end{align}
          \uline{Identify the functions $\mu(\xv)$, $\lambda(\xv)$ and $q(y)$} for the above specialization. Based on the linear parameterization in Ex 2.2, derive the \uline{negative log-likelihood} and \uline{gradient}.  [Hint: Compare with what you already learned about logistic regression.]

          \ans\\
          \leavevmode\\
          The given probability can be rewritten as:
          \begin{equation*}
            \begin{aligned}
              p(y|\xv) = & \exp\left[ y \log \nu(\xv) + (1-y) \log (1-\nu(\xv)) \right]         \\
              =          & \exp\left[ y \frac{\nu(\xv)}{1-\nu(\xv)} + \log (1-\nu(\xv)) \right]
            \end{aligned}
          \end{equation*}

          According to \eqref{eq:GLM}, we identify:
          \begin{align}
            \boxed{\mu(\xv) = \log \left(\frac{\nu(\xv)}{1-\nu(\xv)}\right)}
          \end{align}
          \begin{equation*}
            \begin{aligned}
              \Rightarrow \nu(\xv)= & \frac{1}{1+\exp(-\mu(\xv))}           \\
              =                     & \frac{1}{1+\exp(-\inner{\xbs}{\wbs})} \\
            \end{aligned}
          \end{equation*}

          The term $\lambda(\xv)$ depends on $\xv$:
          \begin{align}
            \boxed{\lambda(\xv) = -\log (1 - \nu(\xv))}
          \end{align}
          The remaining terms that are independent of $\xv$ fall into $q(y)$:
          \begin{align}
            \boxed{q(y) = 1}
          \end{align}

          Negative Log-Likelihood:
          \begin{equation*}
            \begin{aligned}
              \ell_n(\wbs) = & -\sum_{i=1}^n \log p(y_i | \xv_i)                                                                                                        \\
              =              & -\sum_{i=1}^n \left[ y_i \log \nu(\xv_i) + (1-y_i) \log (1-\nu(\xv_i)) \right]                                                           \\
              =              & -\sum_{i=1}^n \left[ y_i \log \frac{1}{1+\exp(-\inner{\xbs_i}{\wbs})} + (1-y_i) \log (1-\frac{1}{1+\exp(-\inner{\xbs_i}{\wbs})}) \right]
            \end{aligned}
          \end{equation*}
          Therefore,
          \begin{align}
            \boxed{\ell_n(\wbs)=\sum_{i=1}^n \log(1+\exp(-\inner{\xbs_i}{\wbs}))+(1-y_i)\inner{\xbs_i}{\wbs}}
          \end{align}
          Gradient:
          \begin{equation*}
            \begin{aligned}
              \nabla \ell_n(\wbs) = & \frac{\partial}{\partial\wbs}\sum_{i=1}^n \log(1+\exp(-\inner{\xbs_i}{\wbs}))+(1-y_i)\inner{\xbs_i}{\wbs} \\
              =                     & \sum_{i=1}^n \frac{-\xbs_i\exp(-\inner{\xbs_i}{\wbs})}{1+\exp(-\inner{\xbs_i}{\wbs})}+\xbs_i(1-y_i)       \\
              =                     & \sum_{i=1}^n -\xbs_i\frac{1+\exp(-\inner{\xbs_i}{\wbs})-1}{1+\exp(-\inner{\xbs_i}{\wbs})}+\xbs_i(1-y_i)   \\
              =                     & \sum_{i=1}^n -\xbs_i\left(1-\frac{1}{1+\exp(-\inner{\xbs_i}{\wbs})}\right)+\xbs_i(1-y_i)                  \\
              =                     & \sum_{i=1}^n \xbs_i\left(\frac{1}{1+\exp(-\inner{\xbs_i}{\wbs})}-y_i\right)
            \end{aligned}
          \end{equation*}
          Thus,
          \begin{align}
            \boxed{\nabla \ell_n(\wbs) = \sum_{i=1}^n \left(\frac{1}{1+\exp(-\inner{\xbs_i}{\wbs})}-y_i\right)\xbs_i}
          \end{align}
          {\vskip0.5cm}
    \item (2 pts) Now let us tackle something new. Let
          \begin{align}
            \Pr(\Ysf = y | \xv) = \frac{[\nu(\xv)]^y}{y!} \exp(-\nu(\xv)), ~~ \mbox{ where } ~~ y = 0, 1, 2, \ldots.
          \end{align}
          \uline{Identify the functions $\mu(\xv)$, $\lambda(\xv)$ and $q(y)$} for the above specialization. Based on the linear parameterization in Ex 2.2, derive the \uline{negative log-likelihood} and \uline{gradient}. [Hint: $\Ysf$ here follows the \href{https://en.wikipedia.org/wiki/Poisson_distribution}{Poisson distribution}, which is useful for modeling integer-valued events, \eg, the number of customers at a given time.]

          \ans\\
          \leavevmode\\
          The given probability can be rewritten as:
          $$
            \Pr(\Ysf = y | \xv) = \exp\left[ y \log \nu(\xv) - \nu(\xv) - \log y! \right].
          $$

          According to \eqref{eq:GLM}, we identify:
          \begin{align}
            \boxed{\mu(\xv) = \log \nu(\xv)}
          \end{align}

          The term $\lambda(\xv)$ is:
          \begin{align}
            \boxed{\lambda(\xv) = \nu(\xv)}
          \end{align}

          The remaining terms that are independent of $\xv$ fall into $q(y)$:
          \begin{align}
            \boxed{q(y) = \frac{1}{y!}}
          \end{align}

          Negative Log-Likelihood:
          \begin{equation*}
            \begin{aligned}
              \ell_n(\wbs) = & -\sum_{i=1}^n \log p(y_i | \xv_i)                                           \\
              =              & -\sum_{i=1}^n \log \left(\frac{[\nu(\xv_i)]^y}{y!} \exp(-\nu(\xv_i))\right) \\
              =              & -\sum_{i=1}^n \left[ y_i \log \nu(\xv_i) - \nu(\xv_i) - \log y_i! \right]
            \end{aligned}
          \end{equation*}

          For the linear parameterization $\nu(\xbs) = \exp(\mu(\xbs))=\exp(\inner{\xbs}{\wbs})$, this becomes:
          \begin{align}
            \boxed{\ell_n(\wbs) = -\sum_{i=1}^n \left[ y_i \inner{\xbs_i}{\wbs} - \exp(\inner{\xbs_i}{\wbs}) - \log y_i! \right]}
          \end{align}

          Gradient:
          \begin{align}
            \boxed{\nabla \ell_n(\wbs) = \sum_{i=1}^n  (\exp(\inner{\xv_i}{\wbs})-y_i) \xv_i}
          \end{align}
  \end{enumerate}
\end{exercise}

\newpage
\begin{exercise}[Ordinal regression (4 pts)]
  In many applications, the ``labels'' have an inherent order. For example, the letter grade $A$ is preferred to $B$, which is preferred to $C$, \etc More generally, consider $c$ ordinal labels $1, 2, \ldots, c$, where we prefer label $k$ than $k+1$, for each $k=1, \ldots, c-1$. [The preference is transitive, \ie, any ``smaller'' label is preferred over a ``larger'' label.]

  \begin{enumerate}

    \item (2 pts) Let us consider $c-1$ \emph{parallel} hyperplanes
          $H_k := \{\xv : \inner{\xv}{\wv} + b_k = 0 \}$, which partition our space into $c$ rectangular regions. We define our prediction as
          \begin{align}
            \hat y \leq k \iff \inner{\xv}{\wv} + b_{k} > 0,
          \end{align}
          or more explicitly,
          \begin{align}
            \hat y = k \iff [\inner{\xv}{\wv} + b_{k} > 0 \mbox{ and } \inner{\xv}{\wv} + b_{k-1} \leq 0 ],
          \end{align}
          where $b_0 := -\infty$ and $b_c := \infty$.

          \begin{center}
            \begin{tikzpicture}
              %\draw[step=1cm,gray,very thin] (-1.9,-1.9) grid (5.9,5.9);
              \draw[thick,<-] (0,0) -- (6,0);
              \foreach \x in {1,2,3}
              \draw (\x cm,5pt) -- (\x cm,-5pt) node[anchor=north] {$b_\x$};
              \node[anchor=north] at (4cm, -1pt) {$\cdots$};
              \draw (5 cm,5pt) -- (5 cm,-5pt) node[anchor=north] {$b_{c-1}$};
              \node[anchor=north] at (0cm, -1pt) {$\wv$};
              \node[anchor=south] at (0.5 cm, -1pt) {$1$};
              \node[anchor=south] at (1.5 cm, -1pt) {$2$};
              \node[anchor=south] at (2.5 cm, -1pt) {$3$};
              \node[anchor=south] at (4.5 cm, -1pt) {$c-1$};
              \node[anchor=south] at (5.5 cm, -1pt) {$c$};
            \end{tikzpicture}
          \end{center}

          The ordering in the labels is now respected, if we constrain $b_1 \leq b_2 \leq \cdots \leq b_{c-1}$:
          \begin{align}
            \hat y \leq k \implies \hat y \leq l, ~~ \forall~ l \geq k.
          \end{align}

          We learn the weights $\wv$ and $b_1, \ldots, b_{c-1}$ by reducing to a sequence of (coupled) binary classifications:
          \begin{align}
            \label{eq:svor-c}
            \min_{\wv, b_1 \leq b_2\leq \cdots \leq b_{c-1}}~ \tfrac{\lambda}{2}\|\wv\|_2^2 + \sum_{k=1}^{c-1} \sum_{i=1}^n \max\{0, 1- (\pred{y_i = k}- \pred{y_i=k+1}) (\inner{\xv_i}{\wv} + b_{k}) \},
          \end{align}
          where recall that $\pred{A}$ is 1 if $A$ is true and 0 otherwise. It is clear that when $c=2$, the above reduces to the familiar soft-margin SVM. \uline{Derive the Lagrangian dual of \eqref{eq:svor-c}}. [If it helps, you may ignore the constraint $b_1 \leq \ldots \leq b_{c-1}$.]

          \ans\\
          $$
            \max\{0, 1- (\pred{y_i = k}- \pred{y_i=k+1}) (\inner{\xv_i}{\wv} + b_{k})\} = \max_{0\leq\alpha\leq1}\alpha(1- (\pred{y_i = k}- \pred{y_i=k+1}) (\inner{\xv_i}{\wv} + b_{k}))
          $$
          $$b_1 \leq b_2\leq \cdots \leq b_{c-1}=b_i\leq b_{i+1}\quad\forall i\in\{1,\ldots,c-2\}$$
          The Lagrangian Dual of the primal problem is:
          $$
            \max_{0\leq\alpha\leq1,~\beta\geq0}\min_{\wv, b}~ \frac{\lambda}{2} \|\wv\|_2^2 + \sum_{k=1}^{c-1} \sum_{i=1}^n \alpha_{ik} \left[1-(\pred{y_i = k}- \pred{y_i=k+1}) (\inner{\xv_i}{\wv} + b_{k})\right]+\sum_{k=1}^{c-2} \beta_k (b_k - b_{k+1})
          $$

          Solving inner unconstrained problem by setting derivative to 0:

          For $\wv$:
          $$
            \frac{\partial}{\partial \wv} = \lambda \wv - \sum_{k=1}^{c-1} \sum_{i=1}^n \alpha_{ik} (\pred{y_i = k}- \pred{y_i=k+1}) \xv_i = 0
          $$
          $$\wv=\dfrac{1}{\lambda}\sum_{k=1}^{c-1} \sum_{i=1}^n \alpha_{ik} (\pred{y_i = k}- \pred{y_i=k+1}) \xv_i$$

          For each $b_k$:
          $$
            \frac{\partial}{\partial b_k} = -\sum_{i=1}^n \alpha_{ik} (\pred{y_i = k}- \pred{y_i=k+1})+\beta_k-\beta_{k-1} = 0,\quad\beta_0=\beta_{c-1}=0
          $$
          $$\beta_k-\beta_{k-1}=\sum_{i=1}^n \alpha_{ik} (\pred{y_i = k}- \pred{y_i=k+1})$$
          Since
          \begin{equation*}
            \begin{aligned}
              \sum_{k=1}^{c-2} \beta_k (b_k - b_{k+1})= & \sum_{k=1}^{c-2} \beta_k b_k-\sum_{k=1}^{c-2} \beta_k b_{k+1}   \\
              =                                         & \sum_{k=1}^{c-2} \beta_k b_k-\sum_{k=2}^{c-1} \beta_{k-1} b_{k} \\
              =                                         & \sum_{k=1}^{c-1} \beta_k b_k-\sum_{k=1}^{c-1} \beta_{k-1} b_{k} \\
              =                                         & \sum_{k=1}^{c-1}b_k(\beta_k-\beta_{k-1}),
            \end{aligned}
          \end{equation*}

          we can plug in $\wv$ and $\beta_k-\beta_{k-1}$ back to the Lagrangian Dual and simplify:

          $$
            \max_{0\leq\alpha\leq1}~ -\frac{1}{2\lambda} \left\|\sum_{k=1}^{c-1} \sum_{i=1}^n \alpha_{ik} (\pred{y_i = k}- \pred{y_i=k+1}) \xv_i\right\|_2^2 + \sum_{k=1}^{c-1} \sum_{i=1}^n \alpha_{ik}.
          $$

          Change to minimization:
          $$
            \boxed{\min_{0\leq\alpha\leq1}~
              \frac{1}{2\lambda} \left\|\sum_{k=1}^{c-1} \sum_{i=1}^n \alpha_{ik}\xv_i (\pred{y_i = k}- \pred{y_i=k+1}) \right\|_2^2 - \sum_{k=1}^{c-1} \sum_{i=1}^n \alpha_{ik}}
          $$
          which is the Lagrangian dual of \eqref{eq:svor-c} as required.
            {\vskip0.5cm}
    \item (2 pts) In the previous formulation, to learn $b_k$, essentially we take class $k$ as positive and class $k+1$ as negative. Can you find a ``better'' alternative? \uline{Write down the formulation}. [Hint: it would be similar to \eqref{eq:svor-c}.]

          \ans\\
          $$
            \boxed{\min_{\wv, b_1 \leq b_2\leq \cdots \leq b_{c-1}}~ \frac{\lambda}{2} \|\wv\|^2 + \sum_{k=1}^{c-1} \sum_{i=1}^n \max\left\{0, 1 - \mathrm{sign}(k-y_i) \left(\inner{\xv_i}{\wv} + b_k\right)\right\}}
          $$

          Here, $\mathrm{sign}(k-y_i) = 1$ if $y_i \leq k$, 0 otherwise.

          By using $\mathrm{sign}(k-y_i)$, each classifier $\inner{\xv_i}{\wv} + b_k$ is not just distinguishing between two consecutive classes but is effectively assessing whether the instance $i$ falls below or above the ordinal threshold $k$. This adjustment means that each boundary $b_k$ isn't just learning to separate $k$ from $k+1$ but is learning to separate $1, 2, \ldots, k$ from $k+1, \ldots, c$.
            {\vskip0.5cm}
  \end{enumerate}
\end{exercise}

\end{document}
